<h3 align="center",style="font-size: 22px;"><a href="" style="color:#9C276A">
VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer</a></h3>
<h5 align="center"> If our project helps you, please give us a star â­ on GitHub to support us. ğŸ™ğŸ™ </h2>
<h5 align="center">
[![arXiv](https://img.shields.io/badge/arXiv-Paper-b31b1b.svg?logo=arXiv)]
[![Website](https://img.shields.io/badge/Website-Project_Page-blue.svg?logo=googlechrome&logoColor=white)]
</h5>

To facilitate reproducibility and future research, we will make our code, models, and the benchmark datasets publicly available in the coming week. 

## ğŸ“¢ Updates

- **[Dec 2025]** ğŸ“… We plan to release the **SafeLIBERO benchmark** on **December 20**.
- **[Dec 9, 2025]** ğŸ”¥ Initial release of the **vlsa-aegis** repository.
