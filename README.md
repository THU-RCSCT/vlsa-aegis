<h3 align="center",style="font-size: 22px;"><a href="" style="color:#9C276A">
VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer</a></h3>
<h5 align="center"> If our project helps you, please give us a star â­ on GitHub to support us. ğŸ™ğŸ™ </h2>
<h5 align="center">

[![arXiv](https://img.shields.io/badge/Arxiv-2511.11891-AD1C18.svg?logo=arXiv)](https://arxiv.org/pdf/2512.11891) 
[![Website](https://img.shields.io/badge/Website-Project_Page-blue.svg?logo=googlechrome&logoColor=white)](https://vlsa-aegis.github.io/)
</h5>

To facilitate reproducibility and future research, we will make our code, models, and the benchmark datasets publicly available in the coming week. 

## ğŸ“¢ Updates

- **[Dec 2025]** ğŸ“… We plan to release the **SafeLIBERO benchmark** on **December 20**.
- **[Dec 9, 2025]** ğŸ”¥ Initial release of the **vlsa-aegis** repository.
